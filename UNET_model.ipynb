{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNET-model",
      "provenance": [],
      "authorship_tag": "ABX9TyPAFgoVqqLxssiEyJUgww7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xoro-o/UNET-PyTorch-doodle/blob/main/UNET_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vffkJhR-TsO5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "    super(DoubleConv,self).__init__()\n",
        "    self.conv  =  nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,3,1,1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels,out_channels,3,1,1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "  def forward(self,X):\n",
        "    return self.conv(X) \n",
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "2NudN1GX9qm9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "  def __init__(self,in_channels=3,out_channels=1,features = [64,128,256,512]):\n",
        "    super(UNET,self).__init__()\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
        "\n",
        "    #Down part of UNET\n",
        "    #here we will define the DOWN module\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels,feature))\n",
        "      in_channels = feature\n",
        "    \n",
        "\n",
        "    #Up part of UNET: \n",
        "    #here we define the UP module\n",
        "\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(feature*2,feature,2,2))\n",
        "      self.ups.append(DoubleConv(feature*2,feature))\n",
        "\n",
        "\n",
        "    # bottleneck part\n",
        "    self.bottleneck  = DoubleConv(features[-1],features[-1]*2)\n",
        "    #final part\n",
        "    self.final = nn.Conv2d(64,1,1)\n",
        "\n",
        "\n",
        "  # here again as we divide delta by 2 it will floor results so we have to cover up our implementation with 2 crop images\n",
        "  def crop_img(tensor,target_tensor):\n",
        "    target_size = target_tensor.size()[2]\n",
        "    tensor_size = tensor.size()[2]\n",
        "    delta = tensor_size-target_size\n",
        "    delta = delta//2\n",
        "    return tensor[:,:,delta:tensor_size-delta-1,delta:tensor_size-delta-1]\n",
        "  def crop_img2(tensor,target_tensor):\n",
        "    target_size = target_tensor.size()[2]\n",
        "    tensor_size = tensor.size()[2]\n",
        "    delta = tensor_size-target_size\n",
        "    delta = delta//2\n",
        "    return tensor[:,:,delta:tensor_size-delta,delta:tensor_size-delta]    \n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    skip_connections = []\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      print(x.size())\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "    \n",
        "    x = self.bottleneck(x)\n",
        "\n",
        "    skip_connections = skip_connections[::-1]\n",
        "    # here my self.ups will be like :\n",
        "    # [conv_transpos1,doubleconv_1,conv_transpose2,doubleconv_2,conv_transpos3,doubleconv_3]\n",
        "    # so here i want to access the transpose part first i.e the indices : 0,2,4,6..\n",
        "    # then i want to access the doubleconv (after the transpose part) i. indices 1,3,5 ..\n",
        "    for idx in range(0,len(self.ups),2):\n",
        "      x = self.ups[idx](x)\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "      \n",
        "\n",
        "      # here we can run into the problem of invalid matching size between the skip_connection dimensions and upsampled transpose convs\n",
        "      # so we can do two things:\n",
        "      # first we can crop and then concatenate\n",
        "      # or we can resize \n",
        "      if(x.shape != skip_connection.shape):\n",
        "        #either we can crop and concatenat using the crop_img function defined above or we can resize\n",
        "        x = transforms.functional.resize(x,size = skip_connection.shape[2:])\n",
        "\n",
        "      concat_skip = torch.cat((skip_connection,x),1)\n",
        "      x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "    return self.final(x)\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "4UscUTWh_8aV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iamge = torch.rand((3,1,160,160))\n",
        "model  = UNET(in_channels = 1,out_channels = 1)\n",
        "pred  = model(iamge)\n",
        "print(model)\n",
        "print(iamge.shape)\n",
        "print(pred.shape)\n",
        "assert iamge.shape == pred.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib0FcS9nZnao",
        "outputId": "3322a85a-3120-44f9-ef38-46f2bc6f217b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64, 160, 160])\n",
            "torch.Size([3, 128, 80, 80])\n",
            "torch.Size([3, 256, 40, 40])\n",
            "torch.Size([3, 512, 20, 20])\n",
            "UNET(\n",
            "  (ups): ModuleList(\n",
            "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (1): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (3): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (5): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (7): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (downs): ModuleList(\n",
            "    (0): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): DoubleConv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bottleneck): DoubleConv(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n",
            "torch.Size([3, 1, 160, 160])\n",
            "torch.Size([3, 1, 160, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mb3jpN6aZ9WW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}